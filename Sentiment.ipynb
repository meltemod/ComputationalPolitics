{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SfX_FhLOI1v1"
   },
   "source": [
    "# Methods for Computational Politics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "aXOGIUVDI1v2"
   },
   "source": [
    "Date: April 17th, 2019<br>Time: 9am - 2:30pm<br>Location: ENR2 S215"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JLPU3ZPnI1v3"
   },
   "source": [
    "## Introduction\n",
    "The 21st century has brought with it a wealth of new methods for data collection and analysis. Due to the convergence of digital trace data availability, more transparent data and code sharing norms, and relatively cheap and plentiful computer processing capabilities, researchers with a laptop and an internet connection can now access a large and growing set of tools and methods. The implications of the \"computational revolution\" is profound for the study of social and political processes, but the skills required to collect and harness these new sources of data are typically not taught to social scientists. This workshop is for anyone who is interested in studying phenomena that are of social and political importance using the growing set of computational methods that are being used to understand these phenomena in new ways."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HToKzi7wI1v4"
   },
   "source": [
    "## Part 2: Sentiment Analysis\n",
    "The second part of the Workshop examines automated processes that researchers can use to understand opinions about a given subject from written language. We will start by learning about sentiment analysis, including what methods are available and how researchers test the validity of the results they obtain. This part of the workshop will cover traditional NLP techniques used to prepare written text for automated analysis (e.g. stemming, tokenization) as well as rule based methods (i.e. using a lexicon) and automated, machine learning methods (i.e. word vectors or word embedding) for conducting sentiment analysis. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "C_7I9N76I1v5"
   },
   "source": [
    "Meltem Odabaş<br>\n",
    "School of Sociology<br>\n",
    "University of Arizona<br>\n",
    "Email: meltemodabas@email.arizona.edu<br>\n",
    "Web: http://www.meltemodabas.net"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NdELahfpI1v6"
   },
   "source": [
    "**bold text**## Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cAWamYG-I1v8"
   },
   "source": [
    "I will not re-invent the wheel for this workshop. I will be using a couple tutorials available online and recycle them for the purpose of our analysis today. If you would like to see those tutorials, please follow the links below.\n",
    "\n",
    "In Section a, we will use a lexicon to do sentiment analysis. In section b, we will create a model that vectorizes each word to estimate tweet sentiments.\n",
    "\n",
    "For Section a, please see: https://www.earthdatascience.org/courses/earth-analytics-python/using-apis-natural-language-processing-twitter/get-and-use-twitter-data-in-python/\n",
    "\n",
    "For Section b, please see: https://www.kaggle.com/varun08/sentiment-analysis-using-word2vec/notebook\n",
    "\n",
    "You can also find additional references at the end of this section."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rCyzDLHP5uda"
   },
   "source": [
    "Before we start coding, however, I would like to tak a bit about different methods of sentiment analysis, using the description here: https://monkeylearn.com/sentiment-analysis/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qYh_pI8MI1v9"
   },
   "source": [
    "**bold text**### a. Sentiment Analysis with TextBlob"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0DRywOAnI1v-"
   },
   "source": [
    "let's start with importing the packages we need for section 4a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 6669,
     "status": "ok",
     "timestamp": 1619304699559,
     "user": {
      "displayName": "Meltem Odabas",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GiIZswGdVRqxUNqDIoizVUcbnhtxOO1YgtPG_J7iw=s64",
      "userId": "01994986109812819812"
     },
     "user_tz": 240
    },
    "id": "MhYdF1XaOoRl",
    "outputId": "38bf06df-bdc1-4e7f-d96c-39bca758f4e4"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting paramiko\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/95/19/124e9287b43e6ff3ebb9cdea3e5e8e88475a873c05ccdf8b7e20d2c4201e/paramiko-2.7.2-py2.py3-none-any.whl (206kB)\n",
      "\u001b[K     |████████████████████████████████| 215kB 5.8MB/s \n",
      "\u001b[?25hCollecting cryptography>=2.5\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/b2/26/7af637e6a7e87258b963f1731c5982fb31cd507f0d90d91836e446955d02/cryptography-3.4.7-cp36-abi3-manylinux2014_x86_64.whl (3.2MB)\n",
      "\u001b[K     |████████████████████████████████| 3.2MB 7.3MB/s \n",
      "\u001b[?25hCollecting bcrypt>=3.1.3\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/26/70/6d218afbe4c73538053c1016dd631e8f25fffc10cd01f5c272d7acf3c03d/bcrypt-3.2.0-cp36-abi3-manylinux2010_x86_64.whl (63kB)\n",
      "\u001b[K     |████████████████████████████████| 71kB 7.1MB/s \n",
      "\u001b[?25hCollecting pynacl>=1.0.1\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/9d/57/2f5e6226a674b2bcb6db531e8b383079b678df5b10cdaa610d6cf20d77ba/PyNaCl-1.4.0-cp35-abi3-manylinux1_x86_64.whl (961kB)\n",
      "\u001b[K     |████████████████████████████████| 962kB 34.4MB/s \n",
      "\u001b[?25hRequirement already satisfied: cffi>=1.12 in /usr/local/lib/python3.7/dist-packages (from cryptography>=2.5->paramiko) (1.14.5)\n",
      "Requirement already satisfied: six>=1.4.1 in /usr/local/lib/python3.7/dist-packages (from bcrypt>=3.1.3->paramiko) (1.15.0)\n",
      "Requirement already satisfied: pycparser in /usr/local/lib/python3.7/dist-packages (from cffi>=1.12->cryptography>=2.5->paramiko) (2.20)\n",
      "Installing collected packages: cryptography, bcrypt, pynacl, paramiko\n",
      "Successfully installed bcrypt-3.2.0 cryptography-3.4.7 paramiko-2.7.2 pynacl-1.4.0\n"
     ]
    }
   ],
   "source": [
    "!pip install paramiko"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 3452,
     "status": "ok",
     "timestamp": 1619304710208,
     "user": {
      "displayName": "Meltem Odabas",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GiIZswGdVRqxUNqDIoizVUcbnhtxOO1YgtPG_J7iw=s64",
      "userId": "01994986109812819812"
     },
     "user_tz": 240
    },
    "id": "7JSDObtaI1v_",
    "outputId": "33eadc98-56e9-457e-c3aa-eec5d3d60ba9"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
      "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import json\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import itertools\n",
    "import collections\n",
    "import os\n",
    "\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import PorterStemmer, WordNetLemmatizer\n",
    "nltk.download('punkt')\n",
    "\n",
    "import tweepy as tw\n",
    "import re\n",
    "import networkx\n",
    "\n",
    "from textblob import TextBlob\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "sns.set(font_scale=1.5)\n",
    "sns.set_style(\"whitegrid\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GTe63OTPI1wE"
   },
   "source": [
    "First, we will open tweets.json data as tweets2 and the convert tweets2 to a dataframe, and extract the text of the tweets only for the analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "executionInfo": {
     "elapsed": 296,
     "status": "ok",
     "timestamp": 1619304748709,
     "user": {
      "displayName": "Meltem Odabas",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GiIZswGdVRqxUNqDIoizVUcbnhtxOO1YgtPG_J7iw=s64",
      "userId": "01994986109812819812"
     },
     "user_tz": 240
    },
    "id": "YRlSH8_kJFCy",
    "outputId": "2e7dde4c-4ca7-4410-cd29-57ac75135443"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.google.colaboratory.intrinsic+json": {
       "type": "string"
      },
      "text/plain": [
       "'/content'"
      ]
     },
     "execution_count": 3,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 20448,
     "status": "ok",
     "timestamp": 1619304770551,
     "user": {
      "displayName": "Meltem Odabas",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GiIZswGdVRqxUNqDIoizVUcbnhtxOO1YgtPG_J7iw=s64",
      "userId": "01994986109812819812"
     },
     "user_tz": 240
    },
    "id": "MR1dQLOrmPDb",
    "outputId": "96de4d4d-1935-4c61-f339-807424fa754b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mounted at /content/drive\n"
     ]
    }
   ],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 303,
     "status": "ok",
     "timestamp": 1619304774414,
     "user": {
      "displayName": "Meltem Odabas",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GiIZswGdVRqxUNqDIoizVUcbnhtxOO1YgtPG_J7iw=s64",
      "userId": "01994986109812819812"
     },
     "user_tz": 240
    },
    "id": "uh1Le5OFm3wR",
    "outputId": "875b5e32-9f73-4c14-e650-0fb4279644da"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 1point6M_tweet\n",
      " Sentiment.ipynb\n",
      "'SICSS kartpostal 2.jpg'\n",
      "'SICSS kartpostal 2.pdf'\n",
      "'SICSS kartpostal 2.psd'\n",
      "'SICSS kartpostal 3.jpg'\n",
      "'SICSS kartpostal 3.pdf'\n",
      "'SICSS kartpostal 3.psd'\n",
      "'Tucson - Places to visit.gmap'\n",
      " tweetcollect.py\n",
      " tweets.json\n",
      " tweets_now.json\n"
     ]
    }
   ],
   "source": [
    "!ls '/content/drive/My Drive'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 432,
     "status": "ok",
     "timestamp": 1619304778074,
     "user": {
      "displayName": "Meltem Odabas",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GiIZswGdVRqxUNqDIoizVUcbnhtxOO1YgtPG_J7iw=s64",
      "userId": "01994986109812819812"
     },
     "user_tz": 240
    },
    "id": "s0beZiGDnL9z",
    "outputId": "d7b89e0b-f03c-45ce-f84f-572a191353b8"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "drive  sample_data\n"
     ]
    }
   ],
   "source": [
    "os.chdir('/content')\n",
    "!ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 208
    },
    "executionInfo": {
     "elapsed": 418,
     "status": "error",
     "timestamp": 1619304782149,
     "user": {
      "displayName": "Meltem Odabas",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GiIZswGdVRqxUNqDIoizVUcbnhtxOO1YgtPG_J7iw=s64",
      "userId": "01994986109812819812"
     },
     "user_tz": 240
    },
    "id": "9GC8B-HvI1wH",
    "outputId": "f0579a8d-5eb4-4e7f-d510-26c7e8c640dc"
   },
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "ignored",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-7-48088e44a770>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Read saved ego tweets\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'tweets.json'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'rb'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mfile\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m     \u001b[0mtweets2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mjson\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfile\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'tweets.json'"
     ]
    }
   ],
   "source": [
    "# Read saved ego tweets\n",
    "with open('tweets.json', 'rb') as file:\n",
    "    tweets2 = json.load(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "-Ey3VurRI1wM"
   },
   "outputs": [],
   "source": [
    "df = pd.DataFrame(tweets2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "2-Yg-zUaI1wP"
   },
   "outputs": [],
   "source": [
    "df = df[['id','text']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "GnXvhXFiI1wV",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df[1:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ZgQTHn_bI1wc"
   },
   "outputs": [],
   "source": [
    "tweetText = list(df['text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "BZ6GBQnvI1wg"
   },
   "outputs": [],
   "source": [
    "tweetText[25:30]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_lVb_LnNI1wl"
   },
   "source": [
    "For this analysis, you only need to remove URLs from the tweets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "C742k-_TI1wn"
   },
   "outputs": [],
   "source": [
    "def remove_url(txt):\n",
    "    \"\"\"Replace URLs found in a text string with nothing \n",
    "    (i.e. it will remove the URL from the string).\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    txt : string\n",
    "        A text string that you want to parse and remove urls.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    The same txt string with url's removed.\n",
    "    \"\"\"\n",
    "\n",
    "    return \" \".join(re.sub(\"([^0-9A-Za-z \\t])|(\\w+:\\/\\/\\S+)\", \"\", txt).split())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "qTwnsJHYI1wq"
   },
   "outputs": [],
   "source": [
    "# Remove URLs\n",
    "tweets_no_urls = [remove_url(tweet) for tweet in tweetText]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "2DNAg0UrI1ww"
   },
   "outputs": [],
   "source": [
    "tweets_no_urls[25:30]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UNdozR-OI1w2"
   },
   "source": [
    "You can use the Python package textblob to calculate the polarity values of individual tweets on ISraElex2019.\n",
    "\n",
    "To learn more about how textblob works, please follow this link: https://planspace.org/20150607-textblob_sentiment/\n",
    "\n",
    "Begin by creating textblob objects, which assigns polarity values to the tweets. You can identify the polarity value using the attribute .polarity of texblob object."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "4Z1cQwrgI1w4"
   },
   "outputs": [],
   "source": [
    "# Create textblob objects of the tweets\n",
    "sentiment_objects = [TextBlob(tweet) for tweet in tweets_no_urls]\n",
    "\n",
    "for i in [25,26,27,28,29,30]:\n",
    "    print(sentiment_objects[i].polarity, sentiment_objects[i])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rUbOLum3I1w8"
   },
   "source": [
    "You can apply list comprehension to create a list of the polarity values and text for each tweet, and then create a Pandas Dataframe from the list."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "xqumFYZ2I1w9"
   },
   "outputs": [],
   "source": [
    "# Create list of polarity values and tweet text\n",
    "sentiment_values = [[tweet.sentiment.polarity, str(tweet)] for tweet in sentiment_objects]\n",
    "\n",
    "sentiment_values[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "f98BoZtfI1xB"
   },
   "outputs": [],
   "source": [
    "# Create dataframe containing the polarity value and tweet text\n",
    "sentiment_df = pd.DataFrame(sentiment_values, columns=[\"polarity\", \"tweet\"])\n",
    "\n",
    "sentiment_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PHeMOXBOI1xJ"
   },
   "source": [
    "These polarity values can be plotted in a histogram, which can help to highlight in the overall sentiment (i.e. more positivity or negativity) toward the subject. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "I5DB84kRI1xK"
   },
   "source": [
    "Because there are retweets, however, I will delete the duplicates first. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "b9EYsyulI1xM"
   },
   "outputs": [],
   "source": [
    "#number of rows before duplicates are removed\n",
    "sentiment_df.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "xjG68PT-I1xR"
   },
   "outputs": [],
   "source": [
    "#Remove duplicates:\n",
    "sentiment_df = sentiment_df.drop_duplicates()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "zYWYs1h0I1xV"
   },
   "outputs": [],
   "source": [
    "#number of rows after duplicates are removed\n",
    "sentiment_df.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "uxjbU2_MI1xd"
   },
   "outputs": [],
   "source": [
    "# Remove polarity values equal to zero\n",
    "sentiment_df = sentiment_df[sentiment_df.polarity != 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "WTzs31Y7I1xo"
   },
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(8, 6))\n",
    "\n",
    "# Plot histogram of the polarity values\n",
    "sentiment_df.hist(bins=[-1, -0.75, -0.5, -0.25, 0.25, 0.5, 0.75, 1],\n",
    "             ax=ax,\n",
    "             color=\"purple\")\n",
    "\n",
    "plt.title(\"Sentiments from Tweets\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uuPRJTalI1xt"
   },
   "source": [
    "### 4.b. Sentiment Analysis with Word Embeddings (word2vec)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HSffHGLa35oT"
   },
   "source": [
    "In the previous section we used a lexicon where each word was assigned with a sentiment polarity value to calculate the overall sentiment of a tweet. Another option is to use a dataset to identify the polarity of sentiment based on a word embeddings model.\n",
    "\n",
    "Please follow this link for a description of word embeddings and word2vec:\n",
    "https://towardsdatascience.com/introduction-to-word-embedding-and-word2vec-652d0c2060fa\n",
    "\n",
    "A word embedding model assigns a vector to each word based on their location in multiple sentences/paragraphs in the text. These vectors represent the meaning of each word. Representing each work by a vector allows us to sum and deduct words as well. For instance, if we can sum and multiply the words king - man + woman, we would expect the result of this equation to be... queen! "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "VVE67NE5I1xu"
   },
   "outputs": [],
   "source": [
    "#load additional packages\n",
    "import numpy as np\n",
    "from bs4 import BeautifulSoup\n",
    "from gensim.models import word2vec\n",
    "tokenizer = nltk.data.load('tokenizers/punkt/english.pickle')\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0nS8k85qI1xx"
   },
   "source": [
    "#### Text Pre-Processing (Tokenization)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OemyIZpcI1xy"
   },
   "source": [
    "Let's start with showing what tokenization is, and how to tokenize a fictitious tweet:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "xbxWsOeGI1xy"
   },
   "outputs": [],
   "source": [
    "tweet = 'RT @meltemodabas: Hello World! This is not a real tweet :D http://example.com/654331/ #Tokenize'\n",
    "print(word_tokenize(tweet))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nYYQ1C4YI1x4"
   },
   "source": [
    "You will notice some peculiarities that are not captured by a general-purpose English tokeniser like the one from NLTK: @-mentions, emoticons, URLs and #hash-tags are not recognised as single tokens. The following code will propose a pre-processing chain that will consider these aspects of the language."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Vya57rgLI1x6"
   },
   "outputs": [],
   "source": [
    "emoticons_str = r\"\"\"\n",
    "    (?:\n",
    "        [:=;] # Eyes\n",
    "        [oO\\-]? # Nose (optional)\n",
    "        [D\\)\\]\\(\\]/\\\\OpP] # Mouth\n",
    "    )\"\"\"\n",
    " \n",
    "regex_str = [\n",
    "    emoticons_str,\n",
    "    r'<[^>]+>', # HTML tags\n",
    "    r'(?:@[\\w_]+)', # @-mentions\n",
    "    r\"(?:\\#+[\\w_]+[\\w\\'_\\-]*[\\w_]+)\", # hash-tags\n",
    "    r'http[s]?://(?:[a-z]|[0-9]|[$-_@.&amp;+]|[!*\\(\\),]|(?:%[0-9a-f][0-9a-f]))+', # URLs\n",
    " \n",
    "    r'(?:(?:\\d+,?)+(?:\\.?\\d+)?)', # numbers\n",
    "    r\"(?:[a-z][a-z'\\-_]+[a-z])\", # words with - and '\n",
    "    r'(?:[\\w_]+)', # other words\n",
    "    r'(?:\\S)' # anything else\n",
    "]\n",
    "    \n",
    "tokens_re = re.compile(r'('+'|'.join(regex_str)+')', re.VERBOSE | re.IGNORECASE)\n",
    "emoticon_re = re.compile(r'^'+emoticons_str+'$', re.VERBOSE | re.IGNORECASE)\n",
    " \n",
    "def tokenize(s):\n",
    "    return tokens_re.findall(s)\n",
    " \n",
    "def preprocess(s, lowercase=False):\n",
    "    tokens = tokenize(s)\n",
    "    if lowercase:\n",
    "        tokens = [token if emoticon_re.search(token) else token.lower() for token in tokens]\n",
    "    return tokens\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "hiDQDIZkI1x-"
   },
   "outputs": [],
   "source": [
    "print(preprocess(tweet))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "i5Hw29NrI1yC"
   },
   "source": [
    "Let's tokenize the first 5 tweets from our search this time. I will use the tweets with removed URLs:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "7Lc1iNR3I1yD"
   },
   "outputs": [],
   "source": [
    "# Tokenize\n",
    "tokenized_tweets= [preprocess(tweet) for tweet in tweets_no_urls]\n",
    "tokenized_tweets[25:30]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PGD4CvtbI1yH"
   },
   "source": [
    "#### Sentiment Analysis with scikit learn and gensim (word2vec)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8L-JE8OWI1yI"
   },
   "source": [
    "This time, rather than using TextBlob, we will use a dataset that is already assigned with sentiment values as our training dataset. We will teach Python what kind of tweets are positive and what kind of tweets are negative using the training dataset. Then, we will ask Python to assign sentiment values to the sentiment we collected (i.e. to the test dataset). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QrgfZIQHI1yJ"
   },
   "source": [
    "I will two datasets available online for the train dataset. They are listed below:\n",
    "\n",
    "i. IMDB reviews with a review score (0=negative, 1=positive) dataset, available here: https://www.kaggle.com/varun08/sentiment-analysis-using-word2vec/data\n",
    "\n",
    "ii. Annotated (0=negative, 4=positive) 1,600,000 tweets extracted, available here: https://www.kaggle.com/kazanova/sentiment140\n",
    "\n",
    "I will run the codes for a 5K version of the 1.6M tweet dataset to show as example. Due to the time constraints, I will then upload the version of full datasets if (i) and (ii) for further analysis."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WWmIVQibI1yK"
   },
   "source": [
    "Let's start with uploading the 1.6M tweets as our \"train\" dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "XheICJfiiWVw"
   },
   "outputs": [],
   "source": [
    "os.chdir('/content/drive/My Drive/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "jJeqlLe-I1yK"
   },
   "outputs": [],
   "source": [
    "DATASET_COLUMNS = [\"target\", \"ids\", \"date\", \"flag\", \"user\", \"text\"]\n",
    "DATASET_ENCODING = \"ISO-8859-1\"\n",
    "train1 = pd.read_csv(\"training.1600000.processed.noemoticon.csv\", encoding =DATASET_ENCODING , names=DATASET_COLUMNS)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "B23V6_8LI1yN"
   },
   "outputs": [],
   "source": [
    "train1[1:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "iVoc8ZFVI1yR"
   },
   "source": [
    "Here, \"target\" is the sentiment score: 0 for negative, 4 for positive. There are other datasets out there that assign 0-1 values to sentiments. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "taPZD6Q6I1yS"
   },
   "source": [
    "Let's reduce this dataset to \"target\",\"ids\" and \"text\", for simplicity:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "IxAK_PnYI1yS"
   },
   "outputs": [],
   "source": [
    "train1 = train1[[\"ids\",\"text\",\"target\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "c0_xuLdGI1yV"
   },
   "outputs": [],
   "source": [
    "train1[1:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sBANHdxh0gt0"
   },
   "source": [
    "Let's look at the total number of rows in this dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "9k9Z1N-7I1yZ"
   },
   "outputs": [],
   "source": [
    "train1.shape[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lsc5xOSUI1ye"
   },
   "source": [
    "This is a huge dataset. Ideal for analysis but for this workshop we do not have much time to parse all 1.6M tweets. What I will do instead is to shrink the dataset to 5K tweets and show how the codes work within the smaller dataset. And then, I will load the versions of the data that I created at home with 1.6M tweets.\n",
    "\n",
    "So, let's shrink this dataset. I will choose 2.5 K tweets with negative and 2.5K tweets with positive sentiment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "NHnRznuQI1ye"
   },
   "outputs": [],
   "source": [
    "train1.groupby('target').count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "fa_88eYJI1yj"
   },
   "outputs": [],
   "source": [
    "temp1 = train1[train1['target'] == 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "myzyo76hI1yn"
   },
   "outputs": [],
   "source": [
    "temp1 = temp1.sample(2500, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "7AFjFLAPI1yt"
   },
   "outputs": [],
   "source": [
    "temp2 = train1[train1['target'] == 4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "L7Iab8StI1yx"
   },
   "outputs": [],
   "source": [
    "temp2 = temp2.sample(2500, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "HPjNptveI1y1"
   },
   "outputs": [],
   "source": [
    "train1_small = temp1.append(temp2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "OMub4lf_76eE"
   },
   "outputs": [],
   "source": [
    "del temp1,temp2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "c076SlLjI1y7",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "train1_small.groupby('target').count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ilontAtdI1zF"
   },
   "source": [
    "let's recall the original dataset we have. We will use this as our test dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "U4uqx9puI1zG",
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "test = df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "wywCrVPUI1zK"
   },
   "outputs": [],
   "source": [
    "test[25:30]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YstAo3GEI1zQ"
   },
   "source": [
    "Note that this is not the format where we cleaned the URLs. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DxXqISt7I1zR"
   },
   "source": [
    "We will use beautifulsoup for text cleaning, and nltk for tokenization (and cleaning the stopwords is an option). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "OWuEp8RdI1zT"
   },
   "outputs": [],
   "source": [
    "# This function converts a text to a sequence of words.\n",
    "def review_wordlist(review, remove_stopwords=False):\n",
    "    # 1. Removing html tags\n",
    "    review_text = BeautifulSoup(review).get_text()\n",
    "    # 2. Removing non-letter.\n",
    "    review_text = re.sub(\"[^a-zA-Z]\",\" \",review_text)\n",
    "    # 3. Converting to lower case and splitting\n",
    "    words = review_text.lower().split()\n",
    "    # 4. Optionally remove stopwords\n",
    "    if remove_stopwords:\n",
    "        stops = set(stopwords.words(\"english\"))     \n",
    "        words = [w for w in words if not w in stops]\n",
    "    \n",
    "    return(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "vjZxw5MaI1zX"
   },
   "outputs": [],
   "source": [
    "# This function splits a review into sentences. \n",
    "def review_sentences(review, tokenizer, remove_stopwords=False):\n",
    "    # 1. Using nltk tokenizer\n",
    "    raw_sentences = tokenizer.tokenize(review.strip())\n",
    "    sentences = []\n",
    "    # 2. Loop for each sentence\n",
    "    for raw_sentence in raw_sentences:\n",
    "        if len(raw_sentence)>0:\n",
    "            sentences.append(review_wordlist(raw_sentence,\\\n",
    "                                            remove_stopwords))\n",
    "    return sentences"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sSl0mc5nI1zb"
   },
   "source": [
    "We will start with train1_small dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "z86rl2B6I1zd"
   },
   "outputs": [],
   "source": [
    "sentences = []\n",
    "print(\"Parsing sentences from training set\")\n",
    "for r in train1_small[\"text\"]:\n",
    "    sentences += review_sentences(r, tokenizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jARd1hug8Zlo"
   },
   "source": [
    "How does the parsed sentences look like? Let's take a look at one example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "thsGnBnD8WeO"
   },
   "outputs": [],
   "source": [
    "sentences[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CLMozwkAI1zi"
   },
   "source": [
    "Now it is time to initialize the train1_small model using the variable sentences:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "mTKInf-ZI1zj"
   },
   "outputs": [],
   "source": [
    "# Importing the built-in logging module\n",
    "import logging\n",
    "logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.INFO)\n",
    "\n",
    "# Creating the model and setting values for the various parameters\n",
    "num_features = 300  # Word vector dimensionality\n",
    "min_word_count = 40 # Minimum word count\n",
    "num_workers = 4     # Number of parallel threads\n",
    "context = 10        # Context window size\n",
    "downsampling = 1e-3 # (0.001) Downsample setting for frequent words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "0F5DzYjkI1zm"
   },
   "outputs": [],
   "source": [
    "# Initializing the train model\n",
    "print(\"Training model....\")\n",
    "model = word2vec.Word2Vec(sentences,\\\n",
    "                          workers=num_workers,\\\n",
    "                          size=num_features,\\\n",
    "                          min_count=min_word_count,\\\n",
    "                          window=context,\n",
    "                          sample=downsampling)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "F8r_5wJKI1zt"
   },
   "source": [
    "If you want to save this model for future use, what you need to do is:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "4hu-qMqBI1zu"
   },
   "outputs": [],
   "source": [
    "# To make the model memory efficient\n",
    "model.init_sims(replace=True)\n",
    "\n",
    "# Saving the model for later use. Can be loaded using Word2Vec.load()\n",
    "model_name = \"model_in_class\"\n",
    "model.save(model_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fqkQNCWbI1zw"
   },
   "source": [
    "Because the dataset here is very small, the model will not have the chance to learn all the words. For example, when we want to run the test, below, we will come up with an error:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "g5op_vGVI1zw",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Few tests: This will print the odd word among them \n",
    "model.wv.doesnt_match(\"man woman dog child kitchen\".split())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Gcp9Mrd5I1z3"
   },
   "source": [
    "Therefore, I am now going to load the model I ran and created at home with 1.6M tweets. \n",
    "Actually, let's make it even more fun: Let's use the model created with 1.6M tweet dataset AND and another model created using an IMDB review dataset of 25K reviews. And compare the two!\n",
    "I will call the IMDB review dataset model as model1, and tweet dataset model as model2.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NgzZylwf8uc8"
   },
   "source": [
    "I did not upload the IMDB dataset yet. Let's upload it and take a look at that dataset before we upload the models I created at home:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "gEvz-RPz6q-D"
   },
   "outputs": [],
   "source": [
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "w_W1WkO78tKB"
   },
   "outputs": [],
   "source": [
    "train2 = pd.read_csv(\"labeledTrainData.tsv\", header=0,\\\n",
    "                    delimiter=\"\\t\", quoting=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "GL8e2-kP970V"
   },
   "outputs": [],
   "source": [
    "train2[0:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ijCGz2jE-DXS"
   },
   "source": [
    "How long is a review? As short as a tweet?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "7cZAVaST-F6P"
   },
   "outputs": [],
   "source": [
    "len(train2['review'][0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "duYVi0_w-Vlc"
   },
   "source": [
    "Oh, not really. So this is a very different dataset. Let's see which one will do better in creating a \"bag of words\" model?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "cODhL7LHI1z4",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model1 = word2vec.Word2Vec.load(\"1point6M_tweet\")\n",
    "model2 = word2vec.Word2Vec.load(\"300features_40minwords_10context\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Wbz0n9n9I1z-",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model1.wv.doesnt_match(\"man woman dog child kitchen\".split())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "CIQH78R_I10B"
   },
   "outputs": [],
   "source": [
    "model2.wv.doesnt_match(\"man woman dog child kitchen\".split())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4eoud0KrI10E"
   },
   "source": [
    "Great! They both work! OK, How about this?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "GDrYPt_XI10E",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model1.wv.doesnt_match(\"france england germany berlin\".split())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "3NRFjYygI10J"
   },
   "outputs": [],
   "source": [
    "model2.wv.doesnt_match(\"france england germany berlin\".split())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Q6XCbigaI10M"
   },
   "source": [
    "We would expect Berlin, right? So that did not work out well for model 1 that comes from the tweet dataset, but works for model 2. \n",
    "\n",
    "Brainstorming: Why do you think this happened to be the case?\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "i7xpxne1-2s9"
   },
   "source": [
    "OK, let's continue. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "whMwQN8dI10M"
   },
   "source": [
    "Can we also see the most similar words to a given word?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "jPGP-wlGI10N"
   },
   "outputs": [],
   "source": [
    "# This will print the most similar words present in the model\n",
    "model1.wv.most_similar(\"man\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "QHXqfOFWI10Q"
   },
   "outputs": [],
   "source": [
    "# This will print the most similar words present in the model\n",
    "model2.wv.most_similar(\"man\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "iqtQU_CjI10T"
   },
   "source": [
    "Or, what does king+woman-man equal to?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "nOTg8WtjI10T",
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "model1.wv.most_similar(positive=['woman', 'king'], negative=['man'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "RR0bY9S-I10V"
   },
   "outputs": [],
   "source": [
    "model2.wv.most_similar(positive=['woman', 'king'], negative=['man'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ltsBgPVtI10b"
   },
   "source": [
    "Yas queen!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "OPphfkCdI10b"
   },
   "outputs": [],
   "source": [
    "# This will give the total number of words in the vocabulary created from this dataset\n",
    "print(model1.wv.syn0.shape)\n",
    "print(model2.wv.syn0.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "el74aZU1_Bjq"
   },
   "source": [
    "**Pop quiz: ** what is 300 here?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "R2JHTwztI10e"
   },
   "source": [
    "Since we calcualted the vectors for each word, now it is the time to calculate tweet-level vectors. To do so, we will take the average of all words appear in a given tweet."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "YAJqjF-_I10i"
   },
   "outputs": [],
   "source": [
    "# Function to average all word vectors in a tweet\n",
    "def featureVecMethod(words, model, num_features):\n",
    "    # Pre-initialising empty numpy array for speed\n",
    "    featureVec = np.zeros(num_features,dtype=\"float32\")\n",
    "    nwords = 0\n",
    "    \n",
    "    #Converting Index2Word which is a list to a set for better speed in the execution.\n",
    "    index2word_set = set(model.wv.index2word)\n",
    "    \n",
    "    for word in  words:\n",
    "        if word in index2word_set:\n",
    "            nwords = nwords + 1\n",
    "            featureVec = np.add(featureVec,model[word])\n",
    "    # Dividing the result by number of words to get average\n",
    "    featureVec = np.divide(featureVec, nwords)\n",
    "    return featureVec    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "bw-gfuhjI10l"
   },
   "outputs": [],
   "source": [
    "# Function for calculating the average feature vector\n",
    "def getAvgFeatureVecs(reviews, model, num_features):\n",
    "    counter = 0\n",
    "    reviewFeatureVecs = np.zeros((len(reviews),num_features),dtype=\"float32\")\n",
    "    for review in reviews:\n",
    "        # Printing a status message every 1000th review\n",
    "        if counter%1000 == 0:\n",
    "            print(\"Review %d of %d\"%(counter,len(reviews)))\n",
    "            \n",
    "        reviewFeatureVecs[counter] = featureVecMethod(review, model, num_features)\n",
    "        counter = counter+1\n",
    "        \n",
    "    return reviewFeatureVecs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9wHQON3wI10o"
   },
   "source": [
    "Are we all convinced that the review dataset is doing a better job by now?\n",
    "\n",
    "No?\n",
    "\n",
    "OK, let's assume we did! :)\n",
    "\n",
    "I will use the review dataset (train2) and the word2vec model created out of that dataset (model2) only, from now on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "AIvZ9z6aI10p"
   },
   "outputs": [],
   "source": [
    "# Calculating average feature vector for training set\n",
    "clean_train_reviews = []\n",
    "for review in train2['review']:\n",
    "    clean_train_reviews.append(review_wordlist(review, remove_stopwords=True))\n",
    "    \n",
    "trainDataVecs = getAvgFeatureVecs(clean_train_reviews, model2, num_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "2fkcvR-oI10s"
   },
   "outputs": [],
   "source": [
    "# Calculating average feature vectors for test set     \n",
    "clean_test_reviews = []\n",
    "for review in test[\"text\"]:\n",
    "    clean_test_reviews.append(review_wordlist(review,remove_stopwords=True))\n",
    "    \n",
    "testDataVecs = getAvgFeatureVecs(clean_test_reviews, model2, num_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "tOHeMYqxI10x"
   },
   "outputs": [],
   "source": [
    "# Fitting a random forest classifier to the training data\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "forest = RandomForestClassifier(n_estimators = 100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "n7jMlZdQB7UV"
   },
   "source": [
    "Ideally, I would run the code below:\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "```\n",
    "print(\"Fitting random forest to training data....\")    \n",
    "forest = forest.fit(trainDataVecs, train2[\"review\"])\n",
    "```\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "However... Time constraints. Let's upload that one as the variable \"forest\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Lz-8l-g7I103"
   },
   "outputs": [],
   "source": [
    "import pickle\n",
    "with open(r'forest.out', \"rb\") as input_file:\n",
    "    forest = pickle.load(input_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "DXV8wnJCGPMi"
   },
   "outputs": [],
   "source": [
    "np.where(np.isnan(testDataVecs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ZrLcI0kSH2Dz"
   },
   "outputs": [],
   "source": [
    "testDataVecs = np.nan_to_num(testDataVecs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "VZ9SOyvII108"
   },
   "outputs": [],
   "source": [
    "# Predicting the sentiment values for test data and saving the results in a csv file \n",
    "result = forest.predict(testDataVecs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Rn-GrFTeI5RL"
   },
   "outputs": [],
   "source": [
    "output = pd.DataFrame(data={\"id\":test[\"id\"],\"review\":test[\"text\"], \"sentiment\":result})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "f9rTf4vvJk9M"
   },
   "outputs": [],
   "source": [
    "output.groupby('sentiment').count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "gU0HcExUI8w-"
   },
   "outputs": [],
   "source": [
    "temp1 = output[output['sentiment'] == 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "FSMV7WxwJtW2"
   },
   "outputs": [],
   "source": [
    "temp1 = temp1.sample(5, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "PvBoqmtAJwX9"
   },
   "outputs": [],
   "source": [
    "temp2 = output[output['sentiment'] == 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "M_9O9iOrJzJ9"
   },
   "outputs": [],
   "source": [
    "temp2 = temp2.sample(5, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "0TlIs7QfJ9CJ"
   },
   "outputs": [],
   "source": [
    "output_small = temp1.append(temp2)\n",
    "del temp1,temp2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "AlAF-_PqKEkH"
   },
   "outputs": [],
   "source": [
    "output_small = list(output_small['review'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "DDARwDAiKpwQ"
   },
   "outputs": [],
   "source": [
    "output_small"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "fB0jFpxkJEnp"
   },
   "outputs": [],
   "source": [
    "output.to_csv( \"output.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2PSlXFMPI11K"
   },
   "source": [
    "### References\n",
    "https://www.earthdatascience.org/courses/earth-analytics-python/using-apis-natural-language-processing-twitter/get-and-use-twitter-data-in-python/\n",
    "\n",
    "https://www.kaggle.com/varun08/sentiment-analysis-using-word2vec/notebook\n",
    "    \n",
    "https://marcobonzanini.com/2015/03/09/mining-twitter-data-with-python-part-2/\n",
    "\n",
    "https://towardsdatascience.com/stemming-lemmatization-what-ba782b7c0bd8\n",
    "\n",
    "https://planspace.org/20150607-textblob_sentiment/\n",
    "\n",
    "https://github.com/sloria/TextBlob/blob/eb08c120d364e908646731d60b4e4c6c1712ff63/textblob/en/en-sentiment.xml\n",
    "\n",
    "https://bhaskarvk.github.io/2015/01/how-to-use-twitters-search-rest-api-most-effectively./\n"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "Sentiment.ipynb",
   "provenance": [
    {
     "file_id": "14fjPlPNDticaGbcGxm87i1mQ7UY1kPe_",
     "timestamp": 1555365691313
    }
   ]
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
